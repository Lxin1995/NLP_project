{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is targeted to the IMDB movie data set. It aims to classify the sentiment of a given review_text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look on the movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nlp_proj_utils import get_imdb_dataset\n",
    "\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already available, skip downloading.\n",
      "imdb loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset, download if necessary\n",
    "train, test = get_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV's \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina's pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.&lt;br /&gt;&lt;br /&gt;Based upon Thomas Rockwell's respected Book, How To Eat Fried Worms starts like any children's story: moving to a new town. The new kid, fifth grader Billy Forrester was once popular, but has to start anew. Making friends is never easy, especially when the only prospect is Poindexter Adam. Or Erica, who...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but 5 additional episodes never aired can be viewed on ABC.com I've watched a lot of television over the years and this is possibly my favorite show, ever. It's a crime that this beautifully written and acted show was canceled. The actors that played Laura, Whit, Carlos, Mae, Damian, Anya and omg, Steven Caseman - are all incredible and so natural in those roles. Even the kids are great. Wonderful show. So sad that it's gone. Of course I wonder abou...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0                                                                                                                                                                                                                         For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.   \n",
       "1  Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV's \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina's pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things ...   \n",
       "2                                                                                                                                                                                       A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.   \n",
       "3  It's a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.<br /><br />Based upon Thomas Rockwell's respected Book, How To Eat Fried Worms starts like any children's story: moving to a new town. The new kid, fifth grader Billy Forrester was once popular, but has to start anew. Making friends is never easy, especially when the only prospect is Poindexter Adam. Or Erica, who...   \n",
       "4  You probably all already know this by now, but 5 additional episodes never aired can be viewed on ABC.com I've watched a lot of television over the years and this is possibly my favorite show, ever. It's a crime that this beautifully written and acted show was canceled. The actors that played Laura, Whit, Carlos, Mae, Damian, Anya and omg, Steven Caseman - are all incredible and so natural in those roles. Even the kids are great. Wonderful show. So sad that it's gone. Of course I wonder abou...   \n",
       "\n",
       "  sentiment  \n",
       "0       pos  \n",
       "1       pos  \n",
       "2       pos  \n",
       "3       pos  \n",
       "4       pos  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (25000, 2)\n",
      "test  shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('train shape:', train.shape)\n",
    "print('test  shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    12500\n",
       "neg    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statics on tags\n",
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transtbl = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Take a text input and return the preprocessed string.\n",
    "    i.e.: preprocessed tokens concatenated by whitespace\n",
    "    \"\"\"\n",
    "    line = line.replace('<br />', '').translate(transtbl)\n",
    "    \n",
    "    # list\n",
    "    tokens = [lemmatizer.lemmatize(t.lower(),'v')  #这个‘v'指的是verb\n",
    "              for t in nltk.word_tokenize(line)   ##也可以写成line.split()\n",
    "              if t.lower() not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/liuxin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be21e84f54dd42e38ba1bfb1fc959f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c88cb8618d41de8edc7a7e2e6f72ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for df in train, test:\n",
    "    df['text_prep'] = df['text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_prep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>Sure, 65 years have passed since Thalberg's last production was filmed. But fellow IMDB members, come on, this movie is surely one of the masterpieces of the 30's! It is a 10.&lt;br /&gt;&lt;br /&gt;This was the first movie I saw at New York's Museum of Modern Art, around 1970 (I was a teenager). Expensive looking yet with scenes of such poverty, masterfully photographed, often thrilling, and always engaging, to me it was MGM movie-making at its best. What did audiences feel when they glimpsed a locust ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>sure 65 years pass since thalberg last production film fellow imdb members come movie surely one masterpieces 30 10 first movie saw new york museum modern art around 1970 teenager expensive look yet scenes poverty masterfully photograph often thrill always engage mgm movie make best audiences feel glimpse locust attack person person destruction mansion horrific poverty splendor wealth last week watch academy award glimpse senior oscar winner attendance luise rainer grand see actress arguably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15782</th>\n",
       "      <td>Sure, it had some of the makings of a good film. The storyline is good, if a bit bland and the acting was good enough though I didn't understand why Olivia d'Abo had such a pronounced Australian accent if her character was supposed to have been raised in the US. My biggest problem, however, was with the wardrobe. I know as rule, the average American is considered a frumpy dresser by any self-respecting European but this was beyond that. Anna's colour combinations were positively ghastly!! An...</td>\n",
       "      <td>neg</td>\n",
       "      <td>sure make good film storyline good bite bland act good enough though understand olivia abo pronounce australian accent character suppose raise us biggest problem however wardrobe know rule average american consider frumpy dresser self respect european beyond anna colour combinations positively ghastly potato sack like sad excuse coat wear throughout film make break hive suppose idea realistic possible many school teachers walk around prada simple mean absolute lack taste word wise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "1473   Sure, 65 years have passed since Thalberg's last production was filmed. But fellow IMDB members, come on, this movie is surely one of the masterpieces of the 30's! It is a 10.<br /><br />This was the first movie I saw at New York's Museum of Modern Art, around 1970 (I was a teenager). Expensive looking yet with scenes of such poverty, masterfully photographed, often thrilling, and always engaging, to me it was MGM movie-making at its best. What did audiences feel when they glimpsed a locust ...   \n",
       "15782  Sure, it had some of the makings of a good film. The storyline is good, if a bit bland and the acting was good enough though I didn't understand why Olivia d'Abo had such a pronounced Australian accent if her character was supposed to have been raised in the US. My biggest problem, however, was with the wardrobe. I know as rule, the average American is considered a frumpy dresser by any self-respecting European but this was beyond that. Anna's colour combinations were positively ghastly!! An...   \n",
       "\n",
       "      sentiment  \\\n",
       "1473        pos   \n",
       "15782       neg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text_prep  \n",
       "1473   sure 65 years pass since thalberg last production film fellow imdb members come movie surely one masterpieces 30 10 first movie saw new york museum modern art around 1970 teenager expensive look yet scenes poverty masterfully photograph often thrill always engage mgm movie make best audiences feel glimpse locust attack person person destruction mansion horrific poverty splendor wealth last week watch academy award glimpse senior oscar winner attendance luise rainer grand see actress arguably...  \n",
       "15782                sure make good film storyline good bite bland act good enough though understand olivia abo pronounce australian accent character suppose raise us biggest problem however wardrobe know rule average american consider frumpy dresser self respect european beyond anna colour combinations positively ghastly potato sack like sad excuse coat wear throughout film make break hive suppose idea realistic possible many school teachers walk around prada simple mean absolute lack taste word wise  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad054cd7685644038cf33baa99c71ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = [w for text in tqdm_notebook(train['text_prep']) \n",
    "             for w in text.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2677\n"
     ]
    }
   ],
   "source": [
    "voca = nltk.FreqDist(all_words)\n",
    "print(voca['sure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 48170),\n",
       " ('movie', 43912),\n",
       " ('one', 26747),\n",
       " ('make', 23538),\n",
       " ('like', 22335),\n",
       " ('see', 20773),\n",
       " ('get', 18108),\n",
       " ('time', 16143),\n",
       " ('good', 15124),\n",
       " ('character', 14153)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords = [word for word, _ in voca.most_common(10000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization / Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train['text_prep'], train['sentiment']\n",
    "test_x, test_y = test['text_prep'], test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use topwords as vocabulary\n",
    "tf_vec = TfidfVectorizer(vocabulary=topwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = tf_vec.fit_transform(train_x)\n",
    "test_features = tf_vec.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.12884974, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0][:50].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### [Multinomial NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for **classification with discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "mnb_model.fit(train_features, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos' 'pos' ... 'neg' 'neg' 'neg']\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "pred = mnb_model.predict(test_features)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.833120\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %f' % metrics.accuracy_score(pred,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.81      0.87      0.84     12500\n",
      "         pos       0.86      0.80      0.83     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass in as keyword arguments to make sure the order is correct\n",
    "print(\n",
    "    metrics.classification_report(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      1.00      0.67         1\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example from sklearn documentation\n",
    "\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(metrics.classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model, vec = train_with_n_topwords(3000, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tf_vec.pkl', 'wb') as fp:\n",
    "    pickle.dump(vec, fp)\n",
    "    \n",
    "with open('mnb_model.pkl', 'wb') as fp:\n",
    "    pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 \n",
    "In this part, we turn to use a neural network architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_proj_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb():\n",
    "    train, test = utils.get_imdb_dataset()\n",
    "    TEXT_COL, LABEL_COL = 'text', 'sentiment'\n",
    "    return (\n",
    "        train[TEXT_COL], train[LABEL_COL],\n",
    "        test[TEXT_COL], test[LABEL_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already available, skip downloading.\n",
      "imdb loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "train_text, train_label, test_text, test_label = load_imdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, # ignore word that only appears in 1 document\n",
    "    ngram_range=(1, 2), # consider both uni-gram and bi-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already available, skip downloading.\n",
      "imdb loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "train_text, train_label, test_text, test_label = load_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn (fit) and transform text into vector\n",
    "train_x = tfidf_vectorizer.fit_transform(train_text)\n",
    "\n",
    "# Convert label to 0 and 1 (optional)\n",
    "train_y = train_label.apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expect 12500 for 1 and 0, instead of pos and neg\n",
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformer to validation set as well\n",
    "# Simply call `transform` this time, don't do `fit` again\n",
    "test_x = tfidf_vectorizer.transform(test_text)\n",
    "test_y = test_label.apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert test_x.shape == train_x.shape\n",
    "assert test_y.shape == train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using `SelectKBest` from `sklearn` and using `f_classif` to help up pick up k best features (word). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 20000 # Dimensions to keep, a hyper parameter\n",
    "\n",
    "# Create a feature selector\n",
    "# By default, f_classif algorithm is used\n",
    "# Other available options include mutual_info_classif, chi2, f_regression etc. \n",
    "\n",
    "selector = SelectKBest(k=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=20000, score_func=<function f_classif at 0x1a3360f730>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to both training data and testing data\n",
    "train_x = selector.transform(train_x)\n",
    "test_x = selector.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x.toarray()\n",
    "test_x=test_x.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Multiple-Layer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_mlp_model(input_dim, layers, output_dim, dropout_rate=0.2):\n",
    "    # Input layer\n",
    "    X = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Hidden layer(s)\n",
    "    H = X\n",
    "    for layer in layers:\n",
    "        H = Dense(layer, activation='relu')(H)\n",
    "        H = Dropout(rate=dropout_rate)(H)\n",
    "    \n",
    "    # Output layer\n",
    "    activation_func = 'softmax' if output_dim > 1 else 'sigmoid'\n",
    "    \n",
    "    Y = Dense(output_dim, activation=activation_func)(H)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'learning_rate': 1e-3,  # default for Adam\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 64,\n",
    "    'layers': [64, 32,16],\n",
    "    'dim': DIM,\n",
    "    'dropout_rate': 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1280064   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,282,689\n",
      "Trainable params: 1,282,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model = build_mlp_model(\n",
    "    input_dim=hyper_params['dim'],\n",
    "    layers=hyper_params['layers'],\n",
    "    output_dim=1,\n",
    "    dropout_rate=hyper_params['dropout_rate'],\n",
    ")\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(\n",
    "    optimizer=Adam(lr=hyper_params['learning_rate']),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using two common callbacks here: `EarlyStopping` and `ModelCheckpoint`. The first is used to prevent overfitting and the second is used to keep track of the best models we got so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stoppping_hook = EarlyStopping(\n",
    "    monitor='val_loss',  # what metrics to track\n",
    "    patience=2,  # maximum number of epochs allowed without imporvement on monitored metrics \n",
    ")\n",
    "\n",
    "CPK_PATH = 'model_cpk.hdf5'    # path to store checkpoint\n",
    "\n",
    "model_cpk_hook = ModelCheckpoint(\n",
    "    CPK_PATH,\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True,  # Only keep the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 13s 531us/sample - loss: 0.4140 - acc: 0.8112 - val_loss: 0.2477 - val_acc: 0.8994\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 12s 497us/sample - loss: 0.1737 - acc: 0.9471 - val_loss: 0.2601 - val_acc: 0.8993\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 12s 484us/sample - loss: 0.1081 - acc: 0.9708 - val_loss: 0.3227 - val_acc: 0.8936\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "his = mlp_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    epochs=10,\n",
    "    validation_data=[test_x, test_y],\n",
    "    batch_size=hyper_params['batch_size'],\n",
    "    callbacks=[early_stoppping_hook, model_cpk_hook],\n",
    ")\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y=mlp_model.predict(\n",
    "    test_x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10,\n",
    "    workers=1, use_multiprocessing=False\n",
    ")\n",
    "predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Load the best model and do evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.load_weights(CPK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "model_root = 'resources/MLP_model'\n",
    "os.makedirs(model_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model structure as json\n",
    "with open(os.path.join(model_root, \"network.json\"), \"w\") as fp:\n",
    "    fp.write(mlp_model.to_json())\n",
    "\n",
    "# Save model weights\n",
    "mlp_model.save_weights(os.path.join(model_root, \"weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 5s 198us/sample - loss: 0.2477 - acc: 0.8994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24774323907375337, 0.89936]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on validation \n",
    "mlp_model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
